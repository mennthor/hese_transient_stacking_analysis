\include{header}

% \addbibresource{references.bib}


\begin{document}

\input{titlepage}



\section{Overview}
This analysis is searching for a stacked lower energy neutrino contribution at the HESE track event positions, which are treated as transient sources.
We test for 21 generic box time windows increasing in logarithmic time from 2 seconds to 5 days symmetrically around all sources.

For the physics motivation: IceCube couldn't find a significant point source so far despite many different analysis and efforts.
Prominent examples relevant for this is search are the \href{time integrated 7 year point source search} and the \href{HESE 6 year point source search}, both with no significant results.
But the HESE events on their own show a clear astrophysical signal and therefore should originate from some sources.
This analysis test if there is any clustering of lower energy neutrino events around the HESE locations, aiming to constrain various HESE emission models.
\textcolor{nordred}{Note: The model selection is currently reviewed}.

The analysis method is a time dependent unbinned likelihood approach similar to the so called \enquote{GRB Likelihood}.
Key features used here are:
\begin{itemize}
  \item Background is modelled using scrambled data
  \item Using time dependent spatial background PDFs
  \item Energy PDF is estimated from data and MC with fixed index $E^{-2}$ power law
\end{itemize}

For source and test data we use:
\begin{itemize}
  \item Source dataset: 6 years HESE track events, IC79 - IC86, 2015.
  \item Test dataset: 5 years PS tracks (IC79 - IC86, 2014) + 1 year GFU (IC86, 2015)
\end{itemize}

The wiki can be found at \url{https://wiki.icecube.wisc.edu/index.php/Transient_HESE_Stacking} and contains this analysis note.



\section{Software}
The analysis scripts are in a git repository and can be found at \path{/home/tmenne/analysis/hese_transient_stacking/analysis}.
Code is enumerated in the order of script execution, if someone wants to redo all analysis steps.
The branch for this analysis is \code{original_hese} which tests the 22 original HESE track events.

The main analysis software used is made from scratch in python with a small C++ back-end for time consuming work with inspiration from \href{http://code.icecube.wisc.edu/projects/icecube/browser/IceCube/sandbox/skylab}{skylab} and \href{http://code.icecube.wisc.edu/projects/icecube/browser/IceCube/sandbox/richman/grbllh}{grbllh}.
The software repository can be found at \path{/home/tmenne/software/tdepps}.
The branch used for the analysis is \code{new_structure}.



\section{Datasets}
This analysis used HESE track events as sources and point source and GFU samples as a test dataset.

\subsection{Sources}
For source positions, the 22 HESE track events from 6 years of HESE data are tested.
The list of events can be seen at the list of \href{https://wiki.icecube.wisc.edu/index.php/Analysis_of_pre-public_alert_HESE/EHE_events#HESE }{pre-public alert HESE events}.
For each of these events detailed \href{http://software.icecube.wisc.edu/documentation/projects/millipede/index.html}{millipede} scans exist.
The best fit positions are taken from these maps for the tested source positions by converting the best fit pixel in local coordinates to equatorial coordinates using the \href{http://software.icecube.wisc.edu/documentation/projects/astro/index.html}{astro} module.

Unlike other point source searches the positions of the sources are not exactly known due to our reconstruction uncertainties.
To estimate the worsened sensitivity for that, the millipede scan maps are later used to inject source positions.
To make this computationally feasible, all maps are converted to equatorial coordinates with the convention $\mathrm{RA} = \phi$ and $\mathrm{DEC} = \pi - \theta$, where $\phi, \theta$ are \href{https://healpy.readthedocs.io/en/latest/}{healpy}'s internal coordinates.
Because of the way healpixels are defined this is not a bijective operation, because the number of pixels in a zenith / declination / $\theta$ band gets smaller to the poles.
So the maps are built by defining a new map with exact pixels in equatorial coordinates, which then get mapped to an interpolated local map.
This can introduce angular errors in the size of a pixel diameter which can be neglected here because of the smoothing described in the next sentence.
As a last step the maps are smoothed with a one degree Gaussian kernel using healpy as it was done for the very first HESE point source search, described \href{https://wiki.icecube.wisc.edu/index.php/High-Energy_Starting_Event_Point_Source_Searches#Effects_of_Binning.2C_Rotation.2C_and_Smoothing}{here}.
This introduces some numerical errors because the smoothing happens in spherical harmonics space which has to be truncated numerically.
The artefacts are removed by setting the resulting map to zero outside the $6\sigma$ level, which is obtained from Wilks' theorem due to a lack of a proper test statistic.
The sum of all smoothed normal space PDF maps can be seen in fig.~(\ref{fig:hese_maps_all}).

\begin{figure}[h]
  \centering
  \includegraphics[width=.9\textwidth]{inc/srcs_summed.png}
  \caption{All 22 HESE scan maps in equatorial coordinates. Each map has been converted from log-Likelihood to normal space and smoothed with a 1 degree Gaussian kernel.}
  \label{fig:hese_maps_all}
\end{figure}


\subsection{Test Data}
To test for a neutrino clustering 6 years of point source track data is used.
For this the standardized datasets from skylab are taken from revision 162239.
Matching to the selection of the HESE sources these include
\begin{itemize}
  \item Point Source Tracks \code{'IC79'}
  \item Point Source Tracks \code{'IC86, 2011'}
  \item Point Source Tracks \code{'IC86, 2012-2014'}
  \item GFU \code{"IC86, 2015"}
\end{itemize}
More information on the PS and GFU datasets can be found at the respective analysis wikis: \href{https://icecube.wisc.edu/~coenders/html/build/html/ic86-bdt/muonL3.html}{PS} and \href{https://icecube.wisc.edu/~tkintscher/internal/gfu_doc/eventselection.html}{GFU}.
The data is split in on- and off-time data for blindness reasons, by cutting out the largest time window around all sources.
Usually for time independent analyses the assumption is, that there is far less signal then data in the sample and scrambled data is used to build PDFs for the LLH.
For the tested, small time windows here, we can go a step further and cut out a little bit of data, not biasing the PDF building process, but making sure no sought after signal is incorporated into it.

The HESE events are removed from the on-data, because they αre the reason to test at these positions.

Monte Carlo simulation files are used corresponding to their data counterparts, also taken from skylab's dataset module.
To avoid biased performance and limit estimation, all HESE-like MC events are removed from the used MC sets.
This is done by running the \href{http://software.icecube.wisc.edu/documentation/projects/VHESelfVeto/index.html}{HESE Veto module} on the original MC \code{i3} files.
The actual paths to the files can be found in the script \path{04-check_hese_mc_ids_jobs.py} in the analysis repository.
Surviving run and event IDs are collected and then matched and removed from the used final level files.

Prepared on-, off- and MC-data are stored under \path{/data/user/tmenne/hese_transient_stacking_analysis/rawout_original_hese}.

To built time dependent Likelihood PDfs, run information is needed.
Because no official runlists that match the event selections could be found, run information is reconstructed from data by using the first and last event per run to estimate the runs livetime.
This underestimates the livetime the more the fewer events are in a run.
If a run only had a single event it was dropped.
This doesn't affect on-time data, only the off-data used to build the model PDFs, so a possible single signal event is not cut away in the on-time data.



\input{method_theory}



\section{Analysis Method – Modelling}
Here we want to show the specific choices made to model the per event PDFs for signal and background contribution.
For each we use a combination of independent PDFs for the spatial, energy and time part.

\subsection{Spatial Part}
For the spatial signal PDF a Kent distribution with the per event circular uncertainty $\sigma$ is used.
\begin{equation}
  f(\Psi | \kappa) = \frac{4\pi\sinh\kappa}{\kappa}
    \exp\left(\kappa (\cos\Psi - 1)\right)
\end{equation}
where $\Psi$ is the space angle between the source and event position n equatorial coordinates.
Instead of a $\sigma$ parameter it takes the shape parameter $\kappa$ which can be related by $\kappa \approx 1 / \sigma^2$ for not too large angles $\lesssim 80^\circ$.
A Kent distribution is practically indistinguishable from a 2D Gaussian for small angular uncertainties but is correctly normalized to the unit sphere.

Background PDFs are constructed in equatorial coordinates as well by assuming a flat distribution in azimuth and are estimated under the assumption that the off-time sample contains only background events.
This is only given for larger time windows as the detector location and orientation together with earth's rotation averages out direct detector geometry effects but matters more for small time windows.
Here this is ignored and a flat azimuthal distribution is assumed.

The PDF is thus only declination dependent and can be written as
\begin{equation}
  f_j(\delta) = \frac{1}{2\pi}P(\delta, t_j)
\end{equation}
where $t_j$ is the source time.
A PDF for each source is built considering the sources time and the event distribution at these times to model the different background behaviour due to seasonal variations.
Fig.~(\ref{fig:rate_all_samples}) shows the seasonal variations for the whole sky for all samples.
The steps are:
\begin{enumerate}
  \item For each sample, the events are binned in $\sin\delta$ in 20 bins, with 14 more dense bins around the horizon region, where the event selection usually switches their selection models.
  \item For each bin use the runlists to build $(x|y)$ pairs in time vs. rate information.
  Then rebin that to monthly bins to average out statistical fluctuation for the fit to come.
  \item Fit a sinus function $f(t) = a \sin(b(t-c)) + d$ with free parameters amplitude $a$ and average rate $d$ to the bins to obtain a rate model for each bin.
  The period $2\pi/b$ and the time offset $c$ are fixed to 365 days and to the offset obtained from a full parameter model fit to the whole dataset to fix the beginning of the seasonal variation period
  \item Fit a continuous spline model to these discrete parameter points to obtain a parameter set per source depending on its declination.
  \item To get the background allsky PDF for each sources time window, rate models selected for a fine grid of parameters per declination are selected and integrated over each sources time window.
  The integral points are modelled by an interpolating spline and its integral is renormalized to $\int_{-1}^{1} \mathrm{spl}(\sin\delta) \d{\sin\delta} = 1$.

See fig.~(\ref{fig:dec_bin_rate_models}) for the fitted rate models, fig.~(\ref{fig:param_splines}) for the parameter splines and fig.~(\ref{fig:model_bg_pdfs}) for the resulting background PDFs.

\begin{figure}[h]
  \centering
  \includegraphics[width=.9\textwidth]{inc/rate_models_per_dec_bin/IC86_2012-2014.png}
  \caption{Rate model fits to declination bins for sample IC86, 2012–2014}
  \label{fig:dec_bin_rate_models}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{subfigure}[c]{0.49\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/param_splines/IC86_2012-2014_amp.png}
    \subcaption{Amplitude spline}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.49\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/param_splines/IC86_2012-2014_base.png}
    \subcaption{Average rate spline}
  \end{subfigure}
  \caption{Rate model parameter splines for amplitude and average rate for sample IC86, 2012–2014. The amplitude spline shows the seasonal variations in the sample.}
  \label{fig:param_splines}
\end{figure}

\begin{figure}[h] % 4 x 3 grid
  \centering
  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_00.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_01.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_02.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_03.png}
  \end{subfigure}

  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_04.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_05.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_06.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_07.png}
  \end{subfigure}

  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_08.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_09.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_10.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_11.png}
  \end{subfigure}

  \caption{Resulting background PDFs for each source in the IC86, 2012--2014 sample. Shown in blue is a spline fit to the grey histogram, which is the averaged PDF over the whole dataset.}
  \label{fig:model_bg_pdfs}
\end{figure}



\end{enumerate}

\subsection{Energy Part}
TODO

\subsection{Time Part}
TODO

\subsection{Fixed Background Estimation and Stacking Weights}
TODO



\section{Analysis Performance}

\subsection{Background Trial Injection}
TODO

\subsection{Signal Trial Injection}
TODO

\subsection{Background Trial Distributions}
TODO

\subsection{Performance}
\subsubsection{Best Case}
TODO
\subsubsection{Realistic Case}
TODO


\section{Analysis Results}



\appendix
\part*{\appendixname}
\section{Presentations}
\begin{description}
  \item[Transients Call] May 22nd, 2017: \href{https://drive.google.com/file/d/0B_Gkg-MCR-1za1RMbjlzTFE0YVU/view}{First Presentation in Transients group}
  \item[Transients Call] June 12th, 2017: \href{https://drive.google.com/file/d/0B_Gkg-MCR-1zTFI3Umg3XzZrSE0/view}{Progress on Software}
  \item[Transients Call] September 18th: \href{https://drive.google.com/file/d/0B_Gkg-MCR-1zR28tTmhBT3VYTGs/view}{Updates on tests on John Felde's NRT analysis}
  \item[Fall Meeting Berlin] October 10th, 2017: \href{https://events.icecube.wisc.edu/getFile.py/access?contribId=37&sessionId=32&resId=0&materialId=slides&confId=90}{First studies on HESE events with PS sample}
  \item[Transients Call] October 30th, 2017: \href{https://drive.google.com/file/d/0B_Gkg-MCR-1zOFdkajczT3JWNUU/view}{Performance and BG trials for all time windows}
  \item[Transients Call] April 30th, 2018: \href{https://drive.google.com/file/d/12vOMOpt1nMrmnBdM_4wV5sMdg0FUJLqF/view}{Update to 6 years of HESE sources}
\end{description}

\section*{Reviewer Q\&A}
\subsection*{Questions from Sandro Kopper -- Collaboration Reviewer}
  \begin{itemize}
      \item Q:
        Concerning the wiki: Better describe spatial PDF, time windows, background injection, rate model fit, used data sets.
      \item A:
        These are addressed with this new version of the analysis documentation.

      \item Q:
        Remove HESE like events from MC data used for performance estimation, because these events would be source positions and not events that are tested for.
      \item A:
        Done.
        I used the \code{VHESelfVeto} module to store all run, event ID combinations of MC events in the \code{i3} files used for the test data sets.
        Then these are removed from the final level MC files used for sensitivity estimation.

      \item Q:
        What happens with the performance if signal is not injected at the best fit HESE positions which are tested?
        The performance should worsen if the sources are not really at the best fit positions.
      \item A:
        Done.
        This is addressed using the healpy injection mode.
        Each trial a new source position is drawn from each prepared millipede scan PDF map and events are injected from these new positions.
        This worsens the sensitivity as expected, the plots and more explanation can be found in the performance section.

      \item Q:
        Make the rate model declination dependent to cover norther, souther sky differences at least.
      \item A:
        Done.
        This was addressed by the method described in the PDF model building section.
        Rate models are fitted to rates in various declination bins and a spline model is used to get a continuous description of the amplitude and average rate per bins.
        These are used to build a background sinus declination PDF for each source separately considering the actual event distribution in the detector at the source's time.

      \item Q:
        Performance for systematics data sets.
      \item A:
        \textcolor{nordorange}{Not done yet}.
        I'm not sure how to do this for the complete analysis.
        For the 7 year PS analysis, the systematics sets were processed for the new selection after IC86 2011 only and also only tested on that subset.
        I could do the same, but that would mean I had to drop all the sources in the IC79 and IC86 sample.
        This is postponed for now, but if someone has some ideas or maybe systematics sets for the missing years, that'd be great.

      \item Q:
        Test time window between the ones actually tested as a cross check.
      \item A:
        \textcolor{nordorange}{Not done yet}, but I think there's not much to see there.
        If required, I can run some BG trials for time windows in between.
  \end{itemize}

\subsection*{Other Questions}
\subsubsection*{Christoph Raab}
  \begin{itemize}
      \item Q (Transients Call, April 30th, 2018):
        What is done with sources that overlap in a different sample?
      \item A:
        There aren't any to consider here, no source is overlapping into another sample even with the largest time window.
        If there would be one, these sources had to be split up to both samples and an additional split weight had to be introduced for injection.
  \end{itemize}

\subsubsection*{Anna Franckowiak}
  \begin{itemize}
      \item Q (Transients Call, April 30th, 2018):
        What motivates the choice for the time windows, why only 5 days, when TXS has shown some 5 months best fit window?
      \item A:
        The windows are chosen somewhat arbitrary.
        The initial decisions to use time windows up to 5 days are that when the analysis started (before TXS), it was not clear how to handle overlapping time windows, so the 5 days provided some buffer to have each source in its unique time window.
        Also, the longer the time window the longer the trial computation takes so this was also a slight factor of not having too large time windows.
  \end{itemize}



\section{Additional Plots}

\begin{figure}[h]
  \centering
  \includegraphics[width=.9\textwidth]{inc/rate_all_samples.png}
  \caption{All runs in the off data samples together with the 22 HESE sources per sample. The rates differ per sample due to different event selections. Seasonal variations cause the sinus like variations per year.}
  \label{fig:rate_all_samples}
\end{figure}


\end{document}