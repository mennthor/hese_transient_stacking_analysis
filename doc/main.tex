\include{header}

% \addbibresource{references.bib}


\begin{document}

\input{titlepage}

\tableofcontents
\newpage


\section{Overview}
This analysis is searching for a stacked lower energy neutrino contribution at the HESE track event positions, which are treated as transient sources.
We test for 21 generic box time windows increasing in logarithmic time from 2 seconds to 5 days symmetrically around all sources.

For the physics motivation: IceCube couldn't find a significant point source so far despite many different analysis and efforts.
Prominent examples relevant for this is search are the \href{time integrated 7 year point source search} and the \href{HESE 6 year point source search}, both with no significant results.
But the HESE events on their own show a clear astrophysical signal and therefore should originate from some sources.
This analysis test if there is any clustering of lower energy neutrino events around the HESE locations, aiming to constrain various HESE emission models.
\textcolor{nordred}{Note: The model selection is currently reviewed}.

The analysis method is a time dependent unbinned likelihood approach similar to the so called \enquote{GRB Likelihood}.
Key features used here are:
\begin{itemize}
  \item Background is modelled using scrambled data
  \item Using time dependent spatial background PDFs
  \item Energy PDF is estimated from data and MC with fixed index $E^{-2}$ power law
\end{itemize}

For source and test data we use:
\begin{itemize}
  \item Source dataset: 6 years HESE track events, IC79 - IC86, 2015.
  \item Test dataset: 5 years PS tracks (IC79 - IC86, 2014) + 1 year GFU (IC86, 2015)
\end{itemize}

The wiki can be found at \url{https://wiki.icecube.wisc.edu/index.php/Transient_HESE_Stacking} and contains this analysis note.



\section{Software}
The analysis scripts are in a git repository and can be found at \path{/home/tmenne/analysis/hese_transient_stacking/analysis}.
Code is enumerated in the order of script execution, if someone wants to redo all analysis steps.
The branch for this analysis is \code{original_hese} which tests the 22 original HESE track events.

The main analysis software used is made from scratch in python with a small C++ back-end for time consuming work with inspiration from \href{http://code.icecube.wisc.edu/projects/icecube/browser/IceCube/sandbox/skylab}{skylab} and \href{http://code.icecube.wisc.edu/projects/icecube/browser/IceCube/sandbox/richman/grbllh}{grbllh}.
The software repository can be found at \path{/home/tmenne/software/tdepps}.
The branch used for the analysis is \code{new_structure}.



\section{Datasets}
This analysis used HESE track events as sources and point source and GFU samples as a test dataset.

\subsection{Sources}
For source positions, the 22 HESE track events from 6 years of HESE data are tested.
The list of events can be seen at the list of \href{https://wiki.icecube.wisc.edu/index.php/Analysis_of_pre-public_alert_HESE/EHE_events#HESE }{pre-public alert HESE events}.
For each of these events detailed \href{http://software.icecube.wisc.edu/documentation/projects/millipede/index.html}{millipede} scans exist.
The best fit positions are taken from these maps for the tested source positions by converting the best fit pixel in local coordinates to equatorial coordinates using the \href{http://software.icecube.wisc.edu/documentation/projects/astro/index.html}{astro} module.

Unlike other point source searches the positions of the sources are not exactly known due to our reconstruction uncertainties.
To estimate the worsened sensitivity for that, the millipede scan maps are later used to inject source positions.
To make this computationally feasible, all maps are converted to equatorial coordinates with the convention $\mathrm{RA} = \phi$ and $\mathrm{DEC} = \pi - \theta$, where $\phi, \theta$ are \href{https://healpy.readthedocs.io/en/latest/}{healpy}'s internal coordinates.
Because of the way healpixels are defined this is not a bijective operation, because the number of pixels in a zenith / declination / $\theta$ band gets smaller to the poles.
So the maps are built by defining a new map with exact pixels in equatorial coordinates, which then get mapped to an interpolated local map.
This can introduce angular errors in the size of a pixel diameter which can be neglected here because of the smoothing described in the next sentence.
As a last step the maps are smoothed with a one degree Gaussian kernel using healpy as it was done for the very first HESE point source search, described \href{https://wiki.icecube.wisc.edu/index.php/High-Energy_Starting_Event_Point_Source_Searches#Effects_of_Binning.2C_Rotation.2C_and_Smoothing}{here}.
This introduces some numerical errors because the smoothing happens in spherical harmonics space which has to be truncated numerically.
The artifacts are removed by setting the resulting map to zero outside the $6\sigma$ level, which is obtained from Wilks' theorem due to a lack of a proper test statistic.
See fig.~(\ref{fig:source_map_handling}) for the smoothing process, the sum of all smoothed normal space PDF maps can be seen in fig.~(\ref{fig:hese_maps_all}).

\begin{figure}[h]
  \centering
  \includegraphics[width=.9\textwidth]{inc/source_maps/summed_maps.png}
  \caption{All 22 HESE scan maps in equatorial coordinates. Each map has been converted from log-Likelihood to normal space and smoothed with a 1 degree Gaussian kernel.}
  \label{fig:hese_maps_all}
\end{figure}

\begin{figure}[h] % 2 x 2 grid
  \centering
  \begin{subfigure}[c]{0.49\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/180108-HESE_map_raw_and_smoothed/HESE_contours_Floyd_raw_lin.png}
    \subcaption{Original scan map in linear scale.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.49\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/180108-HESE_map_raw_and_smoothed/HESE_contours_Floyd_smooth_lin.png}
    \subcaption{Scan map in linear scale smoothed with a $1^\circ$ gaussian kernel.}
  \end{subfigure}

  \begin{subfigure}[c]{0.49\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/source_maps/artifacts/HESE_123326_artifacts.png}
    \subcaption{Smoothing artifacts after healpy smoothing.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.49\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/source_maps/artifacts/HESE_123326.png}
    \subcaption{Truncated map at $6\sigma$ level with no significant smoothing artifacts left.}
  \end{subfigure}

  \caption{Upper row: Original and smoothed version of the scan maps after conversion to equatorial coordinates. Lower row: Removing artifacts from healpy smoothing.}
  \label{fig:source_map_handling}
\end{figure}


\subsection{Test Data}
To test for a neutrino clustering 6 years of point source track data is used.
For this the standardized datasets from skylab are taken from revision 162239.
Matching to the selection of the HESE sources these include
\begin{itemize}
  \item Point Source Tracks \code{'IC79'}
  \item Point Source Tracks \code{'IC86, 2011'}
  \item Point Source Tracks \code{'IC86, 2012-2014'}
  \item GFU \code{"IC86, 2015"}
\end{itemize}
More information on the PS and GFU datasets can be found at the respective analysis wikis: \href{https://icecube.wisc.edu/~coenders/html/build/html/ic86-bdt/muonL3.html}{PS} and \href{https://icecube.wisc.edu/~tkintscher/internal/gfu_doc/eventselection.html}{GFU}.
The data is split in on- and off-time data for blindness reasons, by cutting out the largest time window around all sources.
Usually for time independent analyses the assumption is, that there is far less signal then data in the sample and scrambled data is used to build PDFs for the LLH.
For the tested, small time windows here, we can go a step further and cut out a little bit of data, not biasing the PDF building process, but making sure no sought after signal is incorporated into it.

The HESE events are removed from the on-data, because they αre the reason to test at these positions.

Monte Carlo simulation files are used corresponding to their data counterparts, also taken from skylab's dataset module.
To avoid biased performance and limit estimation, all HESE-like MC events are removed from the used MC sets.
This is done by running the \href{http://software.icecube.wisc.edu/documentation/projects/VHESelfVeto/index.html}{HESE Veto module} on the original MC \code{i3} files.
The actual paths to the files can be found in the script \path{04-check_hese_mc_ids_jobs.py} in the analysis repository.
Surviving run and event IDs are collected and then matched and removed from the used final level files.

Prepared on-, off- and MC-data are stored under \path{/data/user/tmenne/hese_transient_stacking_analysis/rawout_original_hese}.

To built time dependent Likelihood PDfs, run information is needed.
Because no official runlists that match the event selections could be found, run information is reconstructed from data by using the first and last event per run to estimate the runs livetime.
This underestimates the livetime the more the fewer events are in a run.
If a run only had a single event it was dropped.
This doesn't affect on-time data, only the off-data used to build the model PDFs, so a possible single signal event is not cut away in the on-time data.



\input{method_theory}



\section{Analysis Method – Modelling}
Here we want to show the specific choices made to model the per event PDFs for signal and background contribution.
For each we use a combination of independent PDFs for the spatial, energy and time part.
All numerical settings used in the actual analysis code are stored as \code{JSON} files at \path{out_original_hese/settings/<sample_name>.json}.

\subsection{Spatial Part}
For the spatial signal PDF a Kent distribution with the per event circular uncertainty $\sigma$ is used.
\begin{equation}
  f(\Psi | \kappa) = \frac{4\pi\sinh\kappa}{\kappa}
    \exp\left(\kappa (\cos\Psi - 1)\right)
\end{equation}
where $\Psi$ is the space angle between the source and event position n equatorial coordinates.
Instead of a $\sigma$ parameter it takes the shape parameter $\kappa$ which can be related by $\kappa \approx 1 / \sigma^2$ for not too large angles $\lesssim 80^\circ$.
A Kent distribution is practically indistinguishable from a 2D Gaussian for small angular uncertainties but is correctly normalized to the unit sphere.

Background PDFs are constructed in equatorial coordinates as well by assuming a flat distribution in azimuth and are estimated under the assumption that the off-time sample contains only background events.
This is only given for larger time windows as the detector location and orientation together with earth's rotation averages out direct detector geometry effects but matters more for small time windows.
Here this is ignored and a flat azimuthal distribution is assumed.

The PDF is thus only declination dependent and can be written as
\begin{equation}
  f_j(\delta) = \frac{1}{2\pi}P(\delta, t_j)
\end{equation}
where $t_j$ is the source time.
A PDF for each source is built considering the sources time and the event distribution at these times to model the different background behaviour due to seasonal variations.
Fig.~(\ref{fig:rate_all_samples}) shows the seasonal variations for the whole sky for all samples.
The steps are:
\begin{enumerate}
  \item For each sample, the events are binned in $\sin\delta$ in 20 bins, with 14 more dense bins around the horizon region, where the event selection usually switches their selection models.
  \item For each bin use the runlists to build $(x|y)$ pairs in time vs. rate information.
  Then rebin that to monthly bins to average out statistical fluctuation for the fit to come.
  \item Fit a sinus function $f(t) = a \sin(b(t-c)) + d$ with free parameters amplitude $a$ and average rate $d$ to the bins to obtain a rate model for each bin.
  The period $2\pi/b$ and the time offset $c$ are fixed to 365 days and to the offset obtained from a full parameter model fit to the whole dataset to fix the beginning of the seasonal variation period
  \item Fit a continuous spline model to these discrete parameter points to obtain a parameter set per source depending on its declination.
  \item To get the background allsky PDF for each sources time window, rate models selected for a fine grid of parameters per declination are selected and integrated over each sources time window.
  The integral points are modelled by an interpolating spline and its integral is renormalized to $\int_{-1}^{1} \mathrm{spl}(\sin\delta) \d{\sin\delta} = 1$.
\end{enumerate}

See fig.~(\ref{fig:dec_bin_rate_models}) for the fitted rate models, fig.~(\ref{fig:param_splines}) for the parameter splines and fig.~(\ref{fig:model_bg_pdfs}) for the resulting background PDFs.

\begin{figure}[h]
  \centering
  \includegraphics[width=.9\textwidth]{inc/rate_models_per_dec_bin/IC86_2012-2014.png}
  \caption{Rate model fits to declination bins for sample IC86, 2012–2014}
  \label{fig:dec_bin_rate_models}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{subfigure}[c]{0.49\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/param_splines/IC86_2012-2014_amp.png}
    \subcaption{Amplitude spline}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.49\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/param_splines/IC86_2012-2014_base.png}
    \subcaption{Average rate spline}
  \end{subfigure}
  \caption{Rate model parameter splines for amplitude and average rate for sample IC86, 2012–2014. The amplitude spline shows the seasonal variations in the sample.}
  \label{fig:param_splines}
\end{figure}

\begin{figure}[h] % 4 x 3 grid
  \centering
  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_00.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_01.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_02.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_03.png}
  \end{subfigure}

  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_04.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_05.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_06.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_07.png}
  \end{subfigure}

  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_08.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_09.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_10.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.24\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/sindec_splines/IC86_2012-2014_src_11.png}
  \end{subfigure}

  \caption{Resulting background PDFs for each source in the IC86, 2012--2014 sample. Shown in blue is a spline fit to the gray histogram, which is the averaged PDF over the whole dataset. Red is the sampled background declination distribution following the black spline model}
  \label{fig:model_bg_pdfs}
\end{figure}

\subsection{Energy Part}
The energy PDFs are not build on their own but by directly using a ratio of 2D histograms for background and signal energy vs. $\sin\delta$ distributions.
The background histogram is estimated from off-time data, signal from signal MC weighted to fixed power law index 2.
The same binning is used for both histograms, where the $\sin\delta$ bins are the same as used for the spline PDFs.
40 energy bins are chosen to cover the whole MC energy range.
Because we can have bins where either data or MC or both are missing we need to fill empty values for a smooth PDF ratio.
Missing ratio values are conservatively filled by first filling the edges of each column with the highest / lowest PDF ratio per column.
The empty bins are interpolated using the new edge and all valid values until all bins are filled.
To smoothen out some fluctuations from low statistics in the data histogram, the PDF is further required to monotonically decrease from the highest energies downwards per column which avoids unphysical high ratios for low energy proxies
As a last step a \code{scipy.interpolate.RegularGridInterpolator} is used to obtain continuous ratio values for all $\sin\delta$, energy pairs.
See fig.~(\ref{fig:energy_pdf_ratios}) for an example background PDF ratio and the intermediate fill step.

\begin{figure}[h]
  \centering
  \begin{subfigure}[c]{0.49\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/llh_model/energy_pdfs/IC79.png}
    \subcaption{Energy PDF ratio.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.49\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/time_misc/fill_sindec_logE_hist_border_values.png}
    \subcaption{Intermediate step after filling the column edges.}
  \end{subfigure}
  \caption{Energy PDF ratio for sample IC79 and an intermediate step in construction.}
  \label{fig:energy_pdf_ratios}
\end{figure}


\subsection{Time Part}
The time PDFs are simple box models for both signal and background.
For signal this is a generic choice for an unknown emission process.
For background we could more accurately use the fitted rate model for the background PDF.
But as the amplitudes are flat even at the largest time window the PDF would be virtually indistinguishable from a uniform distribution.
So for code simplicity a box model is also used for the background PDF.
This also mean that no extra sensitivity is coming from the temporal term, it is merely there to select event within the time windows.

\subsection{Fixed Background Estimation and Stacking Weights}
As mentioned previously the background event estimations $\braket{n_{B,k}}$ are estimated from data and fixed for the Likelihood.
The values are obtained by integrating the rate model for the whole sky over each sources time window.
The rate model fits can be seen in fig.~(\ref{fig:rate_allsky}).

\begin{figure}[h]
  \centering
  \begin{subfigure}[c]{0.49\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/rate_models_and_rates/IC79.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.49\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/rate_models_and_rates/IC86_2011.png}
  \end{subfigure}

  \begin{subfigure}[c]{0.49\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/rate_models_and_rates/IC86_2012-2014.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.49\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/rate_models_and_rates/IC86_2015.png}
  \end{subfigure}

  \caption{Allsky rate models for each sample. These are integrated over each sources time window to obtain the fixed background event estimations $\braket{n_{B,k}}$ per source.}
  \label{fig:rate_allsky}
\end{figure}

If stacking weights are a-priori fixed they should reflect the expected signal contribution to each source.
Here the weights are chosen to only reflect the detector acceptances for each source corresponding to a $E^{-2}$ power law.
A spline is fitted to a $\sin\delta$ histogram of the signal and a spline is fit to get a continuous representation.
From these splines the weights are constructed per sample.
The multi Likelihood renormalizes the weights to a global sum of 1.
See fig.~(\ref{fig:stacking_src_weights_spl}) for the splines and MC histograms.

\begin{figure}[h]
  \centering
  \begin{subfigure}[c]{0.49\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/llh_model/stacking_src_weights/IC79.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.49\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/llh_model/stacking_src_weights/IC86_2011.png}
  \end{subfigure}

  \begin{subfigure}[c]{0.49\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/llh_model/stacking_src_weights/IC86_2012-2014.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.49\textwidth}
    \includegraphics[width=0.9\textwidth]{inc/llh_model/stacking_src_weights/IC86_2015.png}
  \end{subfigure}

  \caption{MC histograms in $\sin\delta$ weighted to an $E^{-2}$ power law flux normalized to th full $\sin\delta$ range. Stacking weights are computed per sample and renormalized for the multi sample Likelihood.}
  \label{fig:stacking_src_weights_spl}
\end{figure}

\subsection{Note to LLH Minimization}
In the Likelihood minimization we can take advantage of the small background of this analysis for smaller time windows.
Because most of the time only none, a single event or two events take part in the actual likelihood minimization, we can obtain an analytic result of the test statistic fit.
This speeds up calculations and is the most accurate to do.
Below are the analytic solutions for zero, one and two events with the single sample likelihood, where the number of events mean the number of non-zero signal over background ratios.
The same reasoning holds for the multi sample likelihood.

As a reminder, the single sample test statistic that is fitted is
\begin{equation}
  \frac{1}{2}\Lambda
  = -n_S + \sum_{i=1}^N \ln\left(
      \frac{n_S\sum_{k=1}^{N_\text{srcs}}
            w_k^\text{D}w_k^\text{T}S_{i,k}}
           {\Braket{n_{B,k}} B_i} + 1\right)
  = -n_S + \sum_{i=1}^N \ln\left(n_S \cdot R_i + 1\right)
\end{equation}
where $R_i$ is introduced as a shortcut for the fixed signal over background ratios per data configuration, which has the gradient in the single fit parameter $n_S$
\begin{equation}
  \frac{1}{2}\deldel{\lambda}{n_S}
  = -1 + \sum_{i=1}^N \frac{R_i}{n_S\cdot R_i + 1} \mcomma
\end{equation}

For zero event the case is trivial because the \enquote{fit} is directly zero -- we don't allow $n_S < 0$ in this analysis and only fit for over-fluctuations -- because no sum term is surviving
\begin{equation}
  \frac{1}{2}\Lambda = -n_S \Rightarrow \hat{n}_S
  = 0, \hat{\Lambda} = -2 \mperiod
\end{equation}

For a single surviving event, a single sum term is left and we have to solve the linear equation in the gradient and re-insert in the likelihood to obtain the best fit $n_S$ and $\Lambda$.
The best fit $n_S$ is
\begin{equation}
  0 = -1 + \frac{R_1}{n_S\cdot R_1 + 1}
    \Leftrightarrow \hat{n}_S = \frac{R_1 - 1}{R_1}
\end{equation}
which gets us
\begin{equation}
  \frac{1}{2}\hat{\Lambda}
    = -\hat{n}_S + \ln\left( \hat{n}_S \cdot R_1 + 1 \right)
    = -\hat{n}_S + \ln(R_1)
\end{equation}
for the best fit test statistic value.

The last analytic case handled is the one with two events left, which leaves us with a quadratic equation to solve.
For $n_S$ we get
\begin{align}
  0 &= -1 + \frac{R_1}{n_S\cdot R_1 + 1} + \frac{R_2}{n_S\cdot R_2 + 1} \\
  0 &= \Leftrightarrow n_S^2 + n_S \left(\frac{R_1 + R_2}{R_1 R_2} - 1\right) +
       \frac{1}{R_1 R_2} - \frac{R_1 + R_2}{R_1 R_2}
\end{align}
and with the shortcut $\frac{R_1 + R_2}{R_1 R_2} \def \tilde{c}$ the best fit is obtained solving the quadratic equation
\begin{equation}
  \hat{n}_S = —\frac{1}{2}\tilde{c} + 1 + \sqrt{\frac{\tilde{c}^2}{4} + 1 - \frac{1}{R_1 R_2}}
\end{equation}
The negative solution always resembles the fit for $n_S < 0$ and is not considered here.
Re-inserting into $\Lambda$ gives us the best fit which is not further simplified here
\begin{equation}
  \frac{1}{2}\hat{\Lambda} = -\hat{n}_S +
                             \ln\left( \hat{n}_S \cdot R_1 + 1 \right) +
                             \ln\left( \hat{n}_S \cdot R_2 + 1 \right)
\end{equation}

With the help of \code{Mathematica} we can also show, that for two events it is event sufficient to only compute the solution if the sum of both signal over background ratios is $R_1 + R_2 > 1$.
See fig.~(\ref{fig:mathematica_ts}) for the interesting region.
\begin{figure}[h]
  \centering
  \includegraphics[width=.4\textwidth]{inc/analytic_TS_mathematica/analytic_TS_mathematica.png}
  \caption{Region where the best fit test statistic is nonzero for the two-events-left-case. $a, b$ correspond to $R_1, R_2$. Only if the sum $R_1 + R_2 > 1$, the test statistic is non-zero.}
  \label{fig:mathematica_ts}
\end{figure}



\section{Analysis Performance}

\subsection{Background Trial Injection}
To build the background only test statistic needed for p-value calculation, a new random data set is created each trial and fit with the Likelihood test statistic.
Background trials are done independent for each time window and for each window $10^8$ trials are done to get enough non-zero test statistic values in the low background regime (small time windows).
In general scrambled and resampled off-time data is used to estimate the background contribution.

Each trial data set is created as follows:
\begin{itemize}
  \item Draw the events to inject for each source from the fixed background expectations from a Poisson distribution.
  \item For the requested number of events per source, re-sample off-time data events according to the previously build spline PDF per source.
  \item Replace the right-ascension values with random, uniformly distributed values.
  This assumes a flat azimuth distribution, which might not hold to full extend for the smallest time windows, but is neglected here.
  \item Sample new times in MJD from the rate model per source.
  The parameters are obtained from the previously fit parameter splines.
  For the same arguments as used before in the LLH model, we sample uniformly in the time windows instead of doing a costly rejection sampling of the sine function.
  Because of the small amplitude this is virtually the same event for the largest time window.
\end{itemize}

The selected \textcolor{nordred}{\huge hier weiter}

\subsection{Signal Trial Injection}
TODO

\subsection{Background Trial Distributions}
TODO

\subsection{Performance}
\subsubsection{Best Case}
TODO
\subsubsection{Realistic Case}
TODO


\section{Analysis Results}



\appendix
\part*{\appendixname}
\section{Presentations}
\begin{description}
  \item[Transients Call] May 22nd, 2017: \href{https://drive.google.com/file/d/0B_Gkg-MCR-1za1RMbjlzTFE0YVU/view}{First Presentation in Transients group}
  \item[Transients Call] June 12th, 2017: \href{https://drive.google.com/file/d/0B_Gkg-MCR-1zTFI3Umg3XzZrSE0/view}{Progress on Software}
  \item[Transients Call] September 18th: \href{https://drive.google.com/file/d/0B_Gkg-MCR-1zR28tTmhBT3VYTGs/view}{Updates on tests on John Felde's NRT analysis}
  \item[Fall Meeting Berlin] October 10th, 2017: \href{https://events.icecube.wisc.edu/getFile.py/access?contribId=37&sessionId=32&resId=0&materialId=slides&confId=90}{First studies on HESE events with PS sample}
  \item[Transients Call] October 30th, 2017: \href{https://drive.google.com/file/d/0B_Gkg-MCR-1zOFdkajczT3JWNUU/view}{Performance and BG trials for all time windows}
  \item[Transients Call] April 30th, 2018: \href{https://drive.google.com/file/d/12vOMOpt1nMrmnBdM_4wV5sMdg0FUJLqF/view}{Update to 6 years of HESE sources}
  \item[Analysis Call] May 3rd, 2018: \href{https://drive.google.com/file/d/199a-w_JzH4m6RdVKmOb57rWMw4-HDSJj/view}{Analysis review in analysis call}
\end{description}

\section*{Reviewer Q\&A}
\subsection*{Questions from Sandro Kopper -- Collaboration Reviewer}
  \begin{itemize}
      \item Q:
        Concerning the wiki: Better describe spatial PDF, time windows, background injection, rate model fit, used data sets.
      \item A:
        These are addressed with this new version of the analysis documentation.

      \item Q:
        Remove HESE like events from MC data used for performance estimation, because these events would be source positions and not events that are tested for.
      \item A:
        Done.
        I used the \code{VHESelfVeto} module to store all run, event ID combinations of MC events in the \code{i3} files used for the test data sets.
        Then these are removed from the final level MC files used for sensitivity estimation.

      \item Q:
        What happens with the performance if signal is not injected at the best fit HESE positions which are tested?
        The performance should worsen if the sources are not really at the best fit positions.
      \item A:
        Done.
        This is addressed using the healpy injection mode.
        Each trial a new source position is drawn from each prepared millipede scan PDF map and events are injected from these new positions.
        This worsens the sensitivity as expected, the plots and more explanation can be found in the performance section.

      \item Q:
        Make the rate model declination dependent to cover norther, souther sky differences at least.
      \item A:
        Done.
        This was addressed by the method described in the PDF model building section.
        Rate models are fitted to rates in various declination bins and a spline model is used to get a continuous description of the amplitude and average rate per bins.
        These are used to build a background sinus declination PDF for each source separately considering the actual event distribution in the detector at the source's time.

      \item Q:
        Performance for systematics data sets.
      \item A:
        \textcolor{nordorange}{Not done yet}.
        I'm not sure how to do this for the complete analysis.
        For the 7 year PS analysis, the systematics sets were processed for the new selection after IC86 2011 only and also only tested on that subset.
        I could do the same, but that would mean I had to drop all the sources in the IC79 and IC86 sample.
        This is postponed for now, but if someone has some ideas or maybe systematics sets for the missing years, that'd be great.

      \item Q:
        Test time window between the ones actually tested as a cross check.
      \item A:
        \textcolor{nordorange}{Not done yet}, but I think there's not much to see there.
        If required, I can run some BG trials for time windows in between.
  \end{itemize}

\subsection*{Other Questions}
\subsubsection*{Christoph Raab}
  \begin{itemize}
      \item Q (Transients Call, April 30th, 2018):
        What is done with sources that overlap in a different sample?
      \item A:
        There aren't any to consider here, no source is overlapping into another sample even with the largest time window.
        If there would be one, these sources had to be split up to both samples and an additional split weight had to be introduced for injection.
  \end{itemize}

\subsubsection*{Anna Franckowiak}
  \begin{itemize}
      \item Q (Transients Call, April 30th, 2018):
        What motivates the choice for the time windows, why only 5 days, when TXS has shown some 5 months best fit window?
      \item A:
        The windows are chosen somewhat arbitrary.
        The initial decisions to use time windows up to 5 days are that when the analysis started (before TXS), it was not clear how to handle overlapping time windows, so the 5 days provided some buffer to have each source in its unique time window.
        Also, the longer the time window the longer the trial computation takes so this was also a slight factor of not having too large time windows.
  \end{itemize}

\subsubsection*{Mike Richman}
  \begin{itemize}
      \item Q (Slack April 30th, 2018):
        [...] have you considered \emph{not} using the PS selection? Especially, have you considered using more years of GFU?
        [...] GFU was always about looking for transients, so you might get some extra sensitivity for free.
      \item A:
        I talked quickly to Thomas Kintscher who created the GFU sample and it seems that the earlier years are not in an ready to use shape, at least not on a time scale that works for my thesis anymore.
  \end{itemize}



\section{Additional Plots}

\begin{figure}[h]
  \centering
  \includegraphics[width=.9\textwidth]{inc/sources_and_rates/rate_all_samples.png}
  \caption{All runs in the off data samples together with the 22 HESE sources per sample. The rates differ per sample due to different event selections. Seasonal variations cause the sinus like variations per year.}
  \label{fig:rate_all_samples}
\end{figure}


\end{document}