\section{Analysis Method â€“ Theory}
The analysis is done using an unbinned extended Likelihood method.
THe general idea in the IceCube context is well described in these two overview papers: \href{https://arxiv.org/abs/0801.1604}{Methods for point source analysis in high energy neutrino telescopes} and \href{https://arxiv.org/abs/0912.1572v1}{Time-Dependent Point Source Search Methods in High Energy Neutrino Astronomy}.

\subsection{Extended Unbinned Likelihood}
The extended Likelihood and the corresponding logarithmic extended Likelihood function is defined as
\begin{equation}
  \mathcal{L}(\lambda) = \frac{\lambda^N e^{-\lambda}}{N!}\prod_{i=1}^N P_i
  \quad\Rightarrow\quad
    \ln\mathcal{L}(\lambda) = -\lambda+\sum_{i=1}^N \ln(\lambda P_i)
  \mcomma
\end{equation}
where the constant term $\ln(N!)$ is dropped in the logarithmic version.
Here $\lambda$ is the expected number of events and $N$ the number of measured events following a Poisson counting distribution.
The per event model distribution $P_i$, normalized to integral 1, describes the Likelihood of each event under the assumed model and how likely it contributes to the expectation.
The use of the Poisson term is justified by a renormalization of the per event distributions to include the total number of measured events which is not fixed for multiple experiments of the same kind but may fluctuate around an expectation value.

The tested hypotheses are encoded in the description of the models $P$.
To obtain a fairly general expression we can derive the point source special cases from, we can split the expectation model in multiple classes by splitting the expectation and the models accordingly:
\begin{equation}
  \lambda = \sum_{k=1}^{N_\text{classes}} \lambda_k \geq 0
  \mintertext{and}
  P_i = \frac{1}{\sum_{k=1}^{N_\text{classes}} \lambda_k}\cdot
         \sum_{k=1}^{N_\text{classes}} \lambda_k P_{i,k}
  \mperiod
\end{equation}
The single $\lambda_k$ can be negative but their sum must not because it is still a Poisson expectation parameter.
Additionally we normalized the new split model over all classes and arrive at the form
\begin{equation}
  \ln\mathcal{L}(\{\lambda_k\})
  = -\sum_{k=1}^{N_\text{classes}} \lambda_k +
    \sum_{i=1}^N \ln\left(\sum_{k=1}^{N_\text{classes}}
      \lambda_k P_{i,m} \right)
  \mperiod
\end{equation}

To specialize more, we want to test a signal hypothesis against a background one for $N_\text{srcs}$ sources in general per event $i$.
We thus expand the above expression to include $N_\text{srcs}$ signal and $N_\text{srcs}$ background parameters and the corresponding distributions $S_{i,k}$ and $B_{i,k}$:
\begin{equation}
  \ln\mathcal{L}(\{\lambda_{k,S}\}, \{\lambda_{k,B}\})
  = -\sum_{k=1}^{N_\text{srcs}}\left(\lambda_{k,S}+\lambda_{k,B}\right) +
    \sum_{i=1}^N \ln\left(\sum_{k=1}^{N_\text{srcs}}\left(
      \lambda_{k,S} S_{i,k}+\lambda_{k,B} B_{i,k}\right)\right)
  \mcomma
\end{equation}
and from the Poisson condition we still have the constrain
\begin{equation}
  \sum_{k=1}^{N_\text{srcs}}\left(\lambda_{k,S}+\lambda_{k,B}\right) \geq 0
  \mperiod
\end{equation}

For testing the significance of a potential signal contribution in the measured data, a Likelihood ratio test is used.
The null hypotheses $H_0$, which is that only background is expected to be measured, is constructed by using only a portion $\Theta_0$ of the allowed parameter space, here by setting all signal expectations to zero
\begin{equation}
  \ln\mathcal{L}_0(\{\lambda_{k,S}=0\}, \{\lambda_{k,B}\})
  = -\sum_{k=1}^{N_\text{srcs}}\left(\lambda_{k,B}\right) +
    \sum_{i=1}^N \ln\left(\sum_{k=1}^{N_\text{srcs}}\left(
      \lambda_{k,B} B_{i,k}\right)\right)
  \mperiod
\end{equation}
The alternative hypothesis $H_1$ is constructed using the full Likelihood parameter space $\Theta$:
\begin{equation}
  \ln\mathcal{L}_1(\{\lambda_{k,S}\}, \{\lambda_{k,B}\})
  = -\sum_{k=1}^{N_\text{srcs}}\left(\lambda_{k,S}+
                                     \lambda_{k,B}\right) +
    \sum_{i=1}^N \ln\left(\sum_{k=1}^{N_\text{srcs}}\left(
      \lambda_{k,S} S_{i,k}+\lambda_{k,B} B_{i,k}\right)\right)
  \mperiod
\end{equation}

The Likelihood ratio test statistic $\Lambda$ for testing the null hypothesis $H_0$ against the alternative $H_1$ is defined as
\begin{equation}
  \ln\Lambda = \ln\left(\frac{\sup_{\theta \in \Theta_0} \mathcal{L}(\theta)}
                          {\sup_{\theta \in \Theta} \mathcal{L}(\theta)}\right)
  = \ln\left(\sup_{\theta \in \Theta_0} \mathcal{L}(\theta)\right) -
    \ln\left(\sup_{\theta \in \Theta} \mathcal{L}(\theta)\right)
  \mperiod
\end{equation}

Here we introduce $\hat{\lambda}_{k,S/B}$ which mean the parameters $\lambda_{k,S/B}$ that maximize the Likelihood $\mathcal{L}_1$ under the complete parameter space and $\hat{\lambda}_{k,B}^{(0)}$ maximizing $\mathcal{L}_0$.
This leads to the test statistic
\begin{equation}
  \begin{aligned}
    -2\ln\Lambda
    &= 2\ln(\mathcal{L}_1(\{\hat{\lambda}_{k,S/B}\})) -
       2\ln(\mathcal{L}_0(\{\hat{\lambda}^{(0)}_{k,B}\})) \\
    &= -2\left(\sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S} +
                                         \hat{\lambda}_{k,B} -
                                         \hat{\lambda}_{k,S}^{(0)}\right) +
      2\sum_{i=1}^N \ln\left(
        \frac{\sum_{k=1}^{N_\text{srcs}}\left(
            \hat{\lambda}_{k,S} S_{i,k}+\hat{\lambda}_{k,B} B_{i,k}\right)}
            {\sum_{k=1}^{N_\text{srcs}}\left(
              \hat{\lambda}_{k,B}^{(0)} B_{i,k}
            \right)}
          \right)
    \mcomma
  \end{aligned}
\end{equation}
which has been decorated by the factor $-2$ to be compatible to Wilks' theorem.
As seen above we have to distinguish all best fit parameters from both hypotheses in general, differentiating $\hat{\lambda}_{k,B}^{(0)}$ from the null hypothesis which are not the same as $\hat{\lambda}_{k,B}$ from the composite hypothesis.

\subsection{Per event distributions}
The introduced per event distributions $S_i, B_i$ are similar for both the time dependent and independent Likelihoods and follow the conventions for most point source searches in IceCube.
These distributions introduce the separation power between signal and background hypotheses in combination with the mixing portions $\lambda_{i,S/B}$ by introducing a-priori knowledge, defining signal- and background-like regions in the parameter space.
The better these distributions are able to separate signal and background regions the more sensitive the analysis becomes.
A common approach with known good separation power is to combine contributions from spatial clustering and energy information, where the first one necessary for the tested hypotheses and the latter providing additional information under certain assumptions of signal flux shapes.
We can then write the signal and background contributions as
\begin{align}
  S_{i,j} &= S(\vec{x}_i, \vec{x}_{\mathrm{src},j}, E_i | \gamma)
    = S^S(\vec{x}_i, \vec{x}_{\mathrm{src},j})\cdot
      S^E(E_i, \delta_i | \gamma) \\
  \intertext{and}
  B_i &= B(\delta_i, E_i | \phi_\mathrm{BG})
    = B^S(\delta_i) \cdot B^E(E_i, \delta_i | \phi_\mathrm{BG})
\end{align}
where $\gamma$ is the shape parameter of an assumed signal flux proportional to a power law $\propto E^{-\gamma}$ and $\phi_\mathrm{BG}$ the usually better known flux model for an atmospheric neutrino flux.

\subsection{Time dependent Likelihood}
To test for time dependent emission we alter our Likelihood similar to what is usually called \enquote{Gamma Ray Burst Likelihood}.
The assumption is, that transient sources have a well defined time window over which neutrino emission might occur.
Here we additionally use non-overlapping time windows so that each source is unique in its own time window.

To account for that, the signal and background PDFs are chosen to include a time part, which is in the most simple case a rectangle function, having only a non-zero contribution at the source's time windows:
\begin{align}
  S_{i,k}^T &= \rect\left(\frac{t_i - \frac{T_k^1-T_k^0}{2}}
                              {T_k^1-T_k^0}\right) \\
  B_{i,k}^T &= \rect\left(\frac{t_i - \frac{T_k^1-T_k^0}{2}}
                              {T_k^1-T_k^0}\right)
  \mcomma
\end{align}
which will be used later.

The other important simplification is that we don't fit the background expectations, but rather fix them from the integrated off-time data rate over the range of the background time PDFs.
This decreases the number of parameters to fit for, unifies the background estimators
\begin{equation}
  \hat{\lambda}_{k,B} = \hat{\lambda}_{k,B}^{(0)} = \Braket{\lambda_{k,B}}
\end{equation}
and the test statistic turns to
\begin{equation}
  \frac{1}{2}\Lambda
  = -\sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S} +
    \sum_{i=1}^N \ln\left(
      \frac{\sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S} S_{i,k}}
           {\sum_{k=1}^{N_\text{srcs}}\Braket{\lambda_{k,B}} B_{i,k}}
      + 1 \right)
  \mperiod
\end{equation}

In the following we adapt the notation to the standard and use $n$ instead of $\lambda$.

\subsection{Single sample stacking case}

The stacking test statistic for a single sample now has the form
\begin{align}
  \frac{1}{2}\Lambda
  = -\hat{n}_S + \sum_{i=1}^N \ln\left(\frac{\hat{n}_S\bar{S}_i}{\Braket{n_B} B_i} + 1\right) \mcomma
\label{equ:TS}
\end{align}
where $\hat{n}_S$ is the best fit $n_S$ under the full parameter space and $\bar{S}_i$ the stacked signal PDFs per event at each source $k$
\begin{equation}
  \bar{S}_i = \sum_{k=1}^{N_\text{srcs}} w_k^\text{D} w_k^\text{T} S_{i,k} \mperiod
\end{equation}
The detector weights $w^\text{D}$ and intrinsic source weights $w^\text{T}$ are normalized such that
\begin{equation}
  \sum_{k=1}^{N_\text{srcs}} w^\text{D}_k w^\text{T}_k = 1 \mcomma
\end{equation}
where it doesn't matter if $w^\text{D}$ or $w^\text{T}$ have been normalized on their own or not.

For a single sample and assuming non-overlapping source time windows, we can move the sum over the signal PDFs in eq.~(\ref{equ:TS}) outside the logarithm
\begin{align}
  \frac{1}{2}\Lambda
    &= -\hat{n}_S + \sum_{i=1}^N \ln\left(\frac{\hat{n}_S\bar{S}_i}
                                               {\Braket{n_B} B_i} + 1\right) \\
    &= -\hat{n}_S + \sum_{i=1}^N \ln\left(
          \frac{\hat{n}_S\sum_{k=1}^{N_\text{srcs}}
                w_k^\text{D}w_k^\text{T}S_{i,k}}
               {\Braket{n_B} B_i} + 1\right) \mcomma \\
  \intertext{utilizing that each source has it's unique time window, we see that each event $i$ can only contribute to a single source, so that}
    &= -\hat{n}_S + \sum_{i=1}^N \ln\left(
          \frac{\hat{n}_S\sum_{k=1}^{N_\text{srcs}}
                w_k^\text{D}w_k^\text{T}S_{i,k}\delta_{i\in k}}
               {\Braket{n_B} B_i} + 1\right) \mcomma \\
  \intertext{where $\delta_{i\in k}$ is $1$ only if event $i$ falls into the region of source $k$, otherwise $0$, so we get}
    &= -\hat{n}_S + \sum_{i=1}^N \ln\left(
          \frac{\hat{n}_S
                \left[0 + \dots + 0 +
                      w_k^\text{D}w_k^\text{T}S_{i,k} +
                      0 + \dots + 0\right]}
               {\Braket{n_B} B_i} + 1\right) \mperiod \\
  \intertext{Because $\ln(0+1) = \ln(1) = 0$ we can use this to move the sum outside the logarithm}
    &= -\hat{n}_S + \sum_{k=1}^{N_\text{srcs}}\sum_{i=1}^N \ln\left(
          \frac{\hat{n}_S w_k^\text{D}w_k^\text{T}S_{i,k}}
               {\Braket{n_B} B_i} + 1\right)  \label{equ:single_stack} \\
  \intertext{which, as a crosscheck, expands to the correct expression again:}
    &= -\hat{n}_S + \sum_{i=1}^N \ln\left[
          \ln(1) + \dots + \ln(1) +
          \left( \frac{\hat{n}_S w_k^\text{D}w_k^\text{T}S_{i,k}}
                      {\Braket{n_B} B_i} + 1\right) +
          \ln(1) + \dots + \ln(1) \right] \\
    &= -\hat{n}_S + \sum_{i=1}^N \ln\left[ \left(
          \frac{\hat{n}_S w_k^\text{D}w_k^\text{T}S_{i\in k}}
               {\Braket{n_B} B_i} + 1\right) \right] \mperiod
\end{align}

In the following we'll see, that eq.~(\ref{equ:single_stack}) is already the form we can use to stack over multiple samples.


\subsection{Multiple samples stacking case}

Now if we add another sample, we can just sum up the individual Likelihoods, because they are independent:
\begin{align}
  \frac{1}{2}\Lambda
    &= \frac{1}{2} \sum_{j=1}^{N_\text{sam}} \Lambda_j(\hat{n}_{S,j}) \\
    &= \sum_{j=1}^{N_\text{sam}} \left[
        -\hat{n}_{S,j} + \sum_{k\in j}\sum_{i=1}^{N_j} \ln\left(
          \frac{\hat{n}_{S,j} w_{k,j}^\text{D}w_{k,j}^\text{T}S_{i,k,j}}
               {\Braket{n_B} B_{i,j}} + 1
        \right)
      \right] \\
    &=  -\hat{n}_S + \sum_{j=1}^{N_\text{sam}}
          \sum_{k\in j}\sum_{i=1}^{N_j} \ln\left(
            \frac{\hat{n}_S w_j w_{k,j}^\text{D}w_{k,j}^\text{T}S_{i,k,j}}
                 {\Braket{n_B} B_{i,j}} + 1
        \right) \mperiod
\label{equ:multi_TS}
\end{align}
Here we have individual $n_{S,j}$ signal parameters and sources can exclusively fall in only one of the $N_\text{sam}$ samples.
This is indicated by the sum $\sum_{k\in j}$ over sources now only running over all sources included in sample $j$ and the number of events are now the numbers $N_j$ per sample.
Also the PDFs $S_{i,k,j}$ and $B_{i,j}$ are taken with respect to the sample the source and corresponding events fall into because the PDFs can change depending on the event selection, etc.
Finally, $w_{k,j}^\text{D} w_{k,j}^\text{T}$ are now normalized separately within the sample, so that
\begin{equation}
  \sum_{k\in j} w_{k,j}^\text{D} w_{k,j}^\text{T}
  = \sum_{k\in j} \hat{w}_k^\text{D} \hat{w}_k^\text{T} \cdot
    \frac{1}{\sum_{m\in j} \hat{w}_m^\text{D} \hat{w}_m^\text{T}}
  = 1
  \mperiod
  \label{equ:multi_norm}
\end{equation}

In the last step in eq.~(\ref{equ:multi_TS}) we introduced a weight $w_j$ with
\begin{equation}
  \sum_{j=1}^{N_\text{sam}} w_j = 1 \mcomma
\end{equation}
that scales $\hat{n}_S$ to $\hat{n}_{S,j} = \hat{n}_S w_j$.
We need this to insert the correctly scaled single sample LLH parameters $\hat{n}_{S,j}$ which are in general smaller than the combined $\hat{n}_S$.

To obtain the weights $w_j$ we use the law of total probability
\begin{equation}
  w_j = P(j) = \sum_{k=1}^{N_\text{srcs}} P(j| k)\cdot P(k) \mcomma
\label{equ:tot_prob}
\end{equation}
where $P(j)$ is the probability of getting a signal contribution $\hat{n}_S w_j$ from sample $j$.
$P(j|k)$ is the probability of getting signal from source $k$ from sample $j$, normalized over all samples:
\begin{equation}
  \sum_{j=1}^{N_\text{sam}} P(j|k) = 1 \mperiod
\end{equation}
$P(k)$ is the probability of getting signal from source $k$ at all, normalized over all sources separately:
\begin{equation}
  \sum_{k=1}^{N_\text{srcs}} P(k) = 1 \mperiod
\end{equation}
While eq.~(\ref{equ:tot_prob}) holds in more general cases we can use the assumption about our non-overlapping sources to simplify the expression.
We use matrix notation for eq.~(\ref{equ:tot_prob}) to better illustrate that:
\begin{equation}
  \vec{w} =
    \begin{pmatrix}
      P(j=0|k=0) & \dots & P(j=0|k=N_\text{srcs}) \\
      \vdots & \vdots & \vdots \\
      P(j=N_\text{sam}|k=0) & \dots & P(j=N_\text{sam}|k=N_\text{srcs})
    \end{pmatrix} \cdot
    \begin{pmatrix}
      P(k=0) \\ \vdots \\ P(k=N_\text{srcs})
    \end{pmatrix} \mperiod
\end{equation}

Because each source is assumed to be non-overlapping, each column can only have a single entry which is $1$ after normalization per column.
Each row may contain several entries which in sum are then the number of sources per sample.
The probabilities $P(k)$ can be obtained from the detection efficiency of the detector at the source position in the sample they fall into and are normalized over all sources (see eq.~(\ref{equ:multi_example}) for an example):
\begin{equation}
  \sum_{k=1}^{N_\text{srcs}} P(k)
    = \sum_{k=1}^{N_\text{srcs}} w^\text{D}_k w^\text{T}_k \cdot
      \frac{1}{\sum_{m=1}^{N_\text{srcs}} w^\text{D}_m w^\text{T}_m}
    = 1
  \mperiod
\end{equation}

Because the matrix $P(j|k)$ has only entries with either $1$ or $0$, the weights $w_j$ are effectively the sum over the detection efficiencies of all sources falling in sample $j$
\begin{equation}
  w_j = \sum_{k\in j} w^\text{D}_k w^\text{T}_k
        \frac{1}{\sum_{m=1}^{N_\text{srcs}} w^\text{D}_m w^\text{T}_m}
      \mperiod
\end{equation}
$w^\text{D}_k$ and $w^\text{T}_k$ must not be confused with the weights $w_{k,j}^\text{D}$ and $w_{k,j}^\text{T}$ which are normalized per sample $j$.

But the numerators of the weights $w_j$ are now exactly the normalization of the $w_{k,j}^\text{D}$, $w_{k,j}^\text{T}$ in each sample, as previously seen in eq.~(\ref{equ:multi_norm}):
\begin{equation}
  \text{norm}(\{w_{k,j}^\text{D}w_{k,j}^\text{T}\})
    = \sum_{k\in j} w^\text{D}_k w^\text{T}_k
    = w_j \cdot \sum_{k=1}^{N_\text{srcs}} w^\text{D}_k w^\text{T}_k
  \mperiod
\end{equation}

So if we plug the $w_j$ back in to the test statistic (\ref{equ:multi_TS}) we get
\begin{align}
  \frac{1}{2}\Lambda &=
    -\hat{n}_S + \sum_{j=1}^{N_\text{sam}}
            \sum_{k\in j}\sum_{i=1}^{N_j} \ln\left(
              \frac{\hat{n}_S w_j w_{k,j}^\text{D}w_{k,j}^\text{T}S_{i,k,j}}
                   {\Braket{n_B} B_{i,j}} + 1 \right) \\
    &= -\hat{n}_S + \sum_{j=1}^{N_\text{sam}}
            \sum_{k\in j}\sum_{i=1}^{N_j} \ln\left(
              \frac{\hat{n}_S \left[
                      \frac{\sum_{l\in j} w^\text{D}_l w^\text{T}_l }
                           {\sum_{m=1}^{N_\text{srcs}} w^\text{D}_m w^\text{T}_m}
                      \right] w_{k,j}^\text{D}w_{k,j}^\text{T}S_{i,k,j}}
                   {\Braket{n_B} B_{i,j}} + 1 \right)
       \mperiod \\
  \intertext{The numerator of $w_j$ now cancels the per sample normalization of $w_{k,j}^\text{D}$ and $w_{k,j}^\text{T}$ and we get}
    &= -\hat{n}_S + \sum_{j=1}^{N_\text{sam}}
            \sum_{k\in j}\sum_{i=1}^{N_j} \ln\left(
              \frac{\hat{n}_S \left[
                      \frac{ w_k^\text{D}w_k^\text{T} }
                           {\sum_{m=1}^{N_\text{srcs}} w^\text{D}_m w^\text{T}_m}
                      \right] S_{i,k,j}}
                   {\Braket{n_B} B_{i,j}} + 1 \right)
\end{align}

If we now apply the same arguments about mutually exclusive sources as done in the first part, we can combine the sums over $j$ and $k$ and also just sum over all combined events in all samples $i$.
We then get the test statistic for multiple samples and multiple sources as
\begin{equation}
  \frac{1}{2}\Lambda
  = -\hat{n}_S + \sum_{k=1}^{N_\text{srcs}}\sum_{i=1}^{N} \ln\left(
        \frac{\hat{n}_S \left[
                \frac{ w_k^\text{D}w_k^\text{T} }
                     {\sum_{m=1}^{N_\text{srcs}} w^\text{D}_m w^\text{T}_m}
                \right] S_{i,k}}
             {\Braket{n_B} B_{i, k}} + 1 \right)
   \mcomma
   \label{equ:multi_stack}
\end{equation}
where the index $k$ now means that we have to evaluate the PDFs and the weights with the configuration for the specific sample the source is belonging to.
